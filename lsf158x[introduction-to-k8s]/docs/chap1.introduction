==Welcome to LFS158x==

Is your team beginning to use Kubernetes for container orchestration? Do you need guidelines on how to start transforming your organization with Kubernetes and cloud-native patterns? Would you like to simplify software container orchestration and find a way to grow your use of Kubernetes without adding infrastructure complexity? Then this is the course for you!

In this course, we will discuss some of Kubernetes' basic concepts and talk about the architecture of the system, the problems it solves, and the model that it uses to handle containerized deployments and scaling.

This course offers an introduction to Kubernetes and includes technical instructions on how to deploy a stand-alone and multi-tier application. You will learn about ConfigMaps and Secrets, and how to use Ingress.

Upon completion, developers will have a solid understanding of the origin, architecture and building blocks for Kubernetes, and will be able to begin testing the new cloud-native pattern to begin the cloud-native journey.

== Course Learning Objectives ==

By taking this course, you will learn the following:

The origin, architecture, primary components, and building blocks of Kubernetes.
How to set up and access a Kubernetes cluster using Minikube.
Ways to run applications on the deployed Kubernetes environment, and access the deployed applications.
The usefulness of Kubernetes communities, and how you can participate.

===Getting Help===
One great way to interact with peers taking this course and resolving any content-related issues is via the Discussion Forums. These forums can be used in the following ways:

To discuss concepts, tools, and technologies presented in this course, or related to the topics discussed in the course material.
To ask questions about course content.
To share resources and ideas related to Linux.
We strongly encourage you not only to ask questions, but to share with your peers opinions about the course content, as well as valuable related resources. The Discussion Forums will be reviewed periodically by The Linux Foundation staff, but it is primarily a community resource, not an 'ask the instructor' service.

== Course Progress and Completion ==

Once you complete the course (including knowledge check questions and final exam), you will want to know if you have passed. You will be able to see your completion status using the Progress tab at the top of your screen, which will clearly indicate whether or not you have achieved a passing score.

== Introduction ==

With container images, we confine the application code, its runtime, and all of its dependencies in a pre-defined format. And, with container runtimes like runC, containerd, or rkt we can use those pre-packaged images, to create one or more containers. All of these runtimes are good at running containers on a single host. But, in practice, we would like to have a fault-tolerant and scalable solution, which can be achieved by creating a single controller/management unit, after connecting multiple nodes together. This controller/management unit is generally referred to as a container orchestrator.

In this chapter, we will explore why we should use container orchestrators, different implementations of container orchestrators, and where to deploy them.

== Learning Objectives ==

By the end of this chapter, you should be able to:

Define the concept of container orchestration.
Explain the reasons for doing container orchestration.
Discuss different container orchestration options.
Discuss different container orchestration deployment options.

== What Are Containers? ==

Before we dive into container orchestration, let's review first what containers are.
Containers are an application-centric way to deliver high-performing, scalable applications on the infrastructure of your choice.
With a container image, we bundle the application along with its runtime and dependencies. We use that image to create an isolated executable environment, also known as container. We can deploy containers from a given image on the platform of our choice, such as desktops, VMs, cloud, etc.

== What Is Container Orchestration? ==

In the quality assurance (QA) environments, we can get away with running containers on a single host to develop and test applications. However, when we go to production, we do not have the same liberty, as we need to ensure that our applications:

Are fault-tolerant
Can scale, and do this on-demand
Use resources optimally
Can discover other applications automatically, and communicate with each other
Are accessible from the external world
Can update/rollback without any downtime.
Container orchestrators are the tools which group hosts together to form a cluster, and help us fulfill the requirements mentioned above.

== Container Orchestrators ==
Nowadays, there are many container orchestrators available, such as:
Docker Swarm
Kubernetes
Mesos Marathon
Amazon ECS
Hashicorp Nomad

== Why Use Container Orchestrators? ==
Though we can argue that containers at scale can be maintained manually, or with the help of some scripts, container orchestrators can make things easy for operators.
Container orchestrators can:
Bring multiple hosts together and make them part of a cluster
Schedule containers to run on different hosts
Help containers running on one host reach out to containers running on other hosts in the cluster
Bind containers and storage
Bind containers of similar type to a higher-level construct, like services, so we don't have to deal with individual containers
Keep resource usage in-check, and optimize it when necessary
Allow secure access to applications running inside containers.
With all these built-in benefits, it makes sense to use container orchestrators to manage containers. In this course, we will explore Kubernetes.

== Where to Deploy Container Orchestrators ==
Most container orchestrators can be deployed on the infrastructure of our choice. We can deploy them on bare metal, VMs, on-premise, or on a cloud of our choice. For example, Kubernetes can be deployed on our laptop/workstation, inside a company's datacenter, on AWS, on OpenStack, etc. There are even one-click installers available to set up Kubernetes on the cloud, like Google Kubernetes Engine on Google Cloud, or Azure Container Service on Microsoft Azure. Similar solutions are available for other container orchestrators, as well.

There are companies that offer managed Container Orchestration as a Service. We will explore them for Kubernetes in one of the later chapters.

== Learning Objectives (Review) ==
You should now be able to:
Define the concept of container orchestration.
Explain the reasons for doing container orchestration.
Discuss different container orchestration options.
Discuss different container orchestration deployment options.






















